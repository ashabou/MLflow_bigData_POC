{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf25761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/phoenix-hbase-2.5-5.1.3-bin/phoenix-client-hbase-2.5-5.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-tez-0.9.1-bin/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2025-10-21 11:19:21,527 WARN util.Utils: Your hostname, node resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "2025-10-21 11:19:21,534 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2025-10-21 11:19:23,021 INFO spark.SparkContext: Running Spark version 3.4.1\n",
      "2025-10-21 11:19:23,111 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-10-21 11:19:23,355 INFO resource.ResourceUtils: ==============================================================\n",
      "2025-10-21 11:19:23,355 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2025-10-21 11:19:23,355 INFO resource.ResourceUtils: ==============================================================\n",
      "2025-10-21 11:19:23,356 INFO spark.SparkContext: Submitted application: SparkML_MLflow_Test\n",
      "2025-10-21 11:19:23,441 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2025-10-21 11:19:23,470 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2025-10-21 11:19:23,478 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2025-10-21 11:19:23,733 INFO spark.SecurityManager: Changing view acls to: gadet\n",
      "2025-10-21 11:19:23,734 INFO spark.SecurityManager: Changing modify acls to: gadet\n",
      "2025-10-21 11:19:23,735 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2025-10-21 11:19:23,736 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2025-10-21 11:19:23,737 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: gadet; groups with view permissions: EMPTY; users with modify permissions: gadet; groups with modify permissions: EMPTY\n",
      "2025-10-21 11:19:24,310 INFO util.Utils: Successfully started service 'sparkDriver' on port 43507.\n",
      "2025-10-21 11:19:24,401 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2025-10-21 11:19:24,495 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2025-10-21 11:19:24,588 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2025-10-21 11:19:24,590 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2025-10-21 11:19:24,705 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2025-10-21 11:19:24,790 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-16890a48-2307-4a86-9a3b-41bca07c0c24\n",
      "2025-10-21 11:19:24,865 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2025-10-21 11:19:24,924 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2025-10-21 11:19:25,082 INFO util.log: Logging initialized @6986ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2025-10-21 11:19:25,341 INFO ui.JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "2025-10-21 11:19:25,377 INFO server.Server: jetty-9.4.50.v20221201; built: 2022-12-01T22:07:03.915Z; git: da9a0b30691a45daf90a9f17b5defa2f1434f882; jvm 1.8.0_452-8u452-ga~us1-0ubuntu1~20.04-b09\n",
      "2025-10-21 11:19:25,531 INFO server.Server: Started @7435ms\n",
      "2025-10-21 11:19:25,683 INFO server.AbstractConnector: Started ServerConnector@2eb1d787{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2025-10-21 11:19:25,684 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2025-10-21 11:19:25,793 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7346d242{/,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:19:27,238 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2025-10-21 11:19:29,160 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-10-21 11:19:29,161 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-10-21 11:19:29,199 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2025-10-21 11:19:29,199 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2025-10-21 11:19:29,200 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2025-10-21 11:19:29,210 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2025-10-21 11:19:29,221 INFO yarn.Client: Preparing resources for our AM container\n",
      "2025-10-21 11:19:29,301 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2025-10-21 11:19:33,352 INFO yarn.Client: Uploading resource file:/tmp/spark-23cf659d-15d3-4206-ab0e-a5e14d149385/__spark_libs__1980002622798822561.zip -> hdfs://10.0.2.15:9000/user/gadet/.sparkStaging/application_1761038015217_0001/__spark_libs__1980002622798822561.zip\n",
      "2025-10-21 11:19:38,753 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip -> hdfs://10.0.2.15:9000/user/gadet/.sparkStaging/application_1761038015217_0001/pyspark.zip\n",
      "2025-10-21 11:19:38,845 INFO yarn.Client: Uploading resource file:/opt/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip -> hdfs://10.0.2.15:9000/user/gadet/.sparkStaging/application_1761038015217_0001/py4j-0.10.9.7-src.zip\n",
      "2025-10-21 11:19:39,506 INFO yarn.Client: Uploading resource file:/tmp/spark-23cf659d-15d3-4206-ab0e-a5e14d149385/__spark_conf__6524465391294720280.zip -> hdfs://10.0.2.15:9000/user/gadet/.sparkStaging/application_1761038015217_0001/__spark_conf__.zip\n",
      "2025-10-21 11:19:39,599 INFO spark.SecurityManager: Changing view acls to: gadet\n",
      "2025-10-21 11:19:39,600 INFO spark.SecurityManager: Changing modify acls to: gadet\n",
      "2025-10-21 11:19:39,600 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2025-10-21 11:19:39,600 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2025-10-21 11:19:39,600 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: gadet; groups with view permissions: EMPTY; users with modify permissions: gadet; groups with modify permissions: EMPTY\n",
      "2025-10-21 11:19:39,652 INFO yarn.Client: Submitting application application_1761038015217_0001 to ResourceManager\n",
      "2025-10-21 11:19:40,330 INFO impl.YarnClientImpl: Submitted application application_1761038015217_0001\n",
      "2025-10-21 11:19:41,342 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:41,353 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Tue Oct 21 11:19:41 +0200 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1761038379934\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1761038015217_0001/\n",
      "\t user: gadet\n",
      "2025-10-21 11:19:42,360 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:43,363 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:44,367 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:45,373 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:46,376 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:47,383 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 11:19:48,387 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:49,395 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:50,401 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:51,408 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:52,413 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:53,418 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:54,427 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:55,431 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:56,435 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:57,440 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:58,444 INFO yarn.Client: Application report for application_1761038015217_0001 (state: ACCEPTED)\n",
      "2025-10-21 11:19:59,350 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> localhost, PROXY_URI_BASES -> http://localhost:8088/proxy/application_1761038015217_0001), /proxy/application_1761038015217_0001\n",
      "2025-10-21 11:19:59,451 INFO yarn.Client: Application report for application_1761038015217_0001 (state: RUNNING)\n",
      "2025-10-21 11:19:59,452 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.0.2.15\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1761038379934\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://localhost:8088/proxy/application_1761038015217_0001/\n",
      "\t user: gadet\n",
      "2025-10-21 11:19:59,455 INFO cluster.YarnClientSchedulerBackend: Application application_1761038015217_0001 has started running.\n",
      "2025-10-21 11:19:59,487 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34371.\n",
      "2025-10-21 11:19:59,487 INFO netty.NettyBlockTransferService: Server created on 10.0.2.15:34371\n",
      "2025-10-21 11:19:59,498 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2025-10-21 11:19:59,542 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34371, None)\n",
      "2025-10-21 11:19:59,574 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34371 with 366.3 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34371, None)\n",
      "2025-10-21 11:19:59,610 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34371, None)\n",
      "2025-10-21 11:19:59,620 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34371, None)\n",
      "2025-10-21 11:20:00,341 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7346d242{/,null,STOPPED,@Spark}\n",
      "2025-10-21 11:20:00,343 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,356 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a8b8c56{/jobs,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,357 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,360 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49ab1cf5{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,360 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,362 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27cf19{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,384 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,387 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b49d81a{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,389 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,391 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bf0b5f2{/stages,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,394 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,395 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f887ef{/stages/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,399 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,401 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29c1959c{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,404 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,416 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f60de77{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,429 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,455 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2659cf4c{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,457 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,458 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bf4cb04{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,465 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,466 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@471161c2{/storage,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,478 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,490 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@268ed189{/storage/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,491 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,492 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6384d66f{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,492 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,500 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35339f00{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,500 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,514 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5857c949{/environment,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,514 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,515 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bd9067c{/environment/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,516 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,522 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4690b5b1{/executors,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,524 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,528 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b09b8df{/executors/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,532 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,538 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@395f0a71{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,540 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 11:20:00,544 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@612240d0{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,549 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,665 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c6f63b7{/static,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,666 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,687 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71b770a5{/,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,688 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,726 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f45a683{/api,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,727 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,728 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b895ec7{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,728 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,733 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b28adc9{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,744 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:00,746 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77090ab0{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:00,747 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2025-10-21 11:20:01,573 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 11:20:02,192 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.\n",
      "2025-10-21 11:20:02,196 INFO internal.SharedState: Warehouse path is 'hdfs://10.0.2.15:9000/user/hive/warehouse'.\n",
      "2025-10-21 11:20:02,242 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:02,245 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c8632f5{/SQL,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:02,245 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:02,246 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3464aa34{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:02,246 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:02,247 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55ec1e3c{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:02,248 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:02,250 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39f1e48{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:02,252 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2025-10-21 11:20:02,254 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3520df71{/static/sql,null,AVAILABLE,@Spark}\n",
      "2025-10-21 11:20:12,322 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.2.15:45758) with ID 1,  ResourceProfileId 0\n",
      "2025-10-21 11:20:12,409 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.2.15:45754) with ID 2,  ResourceProfileId 0\n",
      "2025-10-21 11:20:13,289 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:33079 with 366.3 MiB RAM, BlockManagerId(2, localhost, 33079, None)\n",
      "2025-10-21 11:20:13,374 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:39903 with 366.3 MiB RAM, BlockManagerId(1, localhost, 39903, None)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkML_MLflow_Test\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "mlflow.set_experiment(\"02 - Spark MLlib\")\n",
    "\n",
    "# HDFS client\n",
    "client = InsecureClient('http://10.0.2.15:9870', user='gadet')\n",
    "print(client.list('/user/gadet/mlflow/artifacts'))\n",
    "\n",
    "\n",
    "def log_artifact_and_backup(local_path, hdfs_dir, client):\n",
    "    mlflow.log_artifact(local_path)\n",
    "    file_name = os.path.basename(local_path)\n",
    "    hdfs_path = os.path.join(hdfs_dir, file_name)\n",
    "    client.upload(hdfs_path, local_path, overwrite=True)\n",
    "    print(f\"Uploaded backup to HDFS: {hdfs_path}\")\n",
    "    \n",
    "def log_confusion_matrix(pred_df, run_name):\n",
    "    y_true = np.array(pred_df.select(\"label\").collect()).flatten()\n",
    "    y_pred = np.array(pred_df.select(\"prediction\").collect()).flatten()\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix: {run_name}\")\n",
    "\n",
    "    artifact_path = f\"{run_name}_confusion_matrix.png\"\n",
    "    plt.savefig(artifact_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    log_artifact_and_backup(artifact_path, f\"/user/gadet/mlflow/artifacts/{run_name}\", client)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821de570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "df[\"label\"] = y\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Train size:\", train_df.count(), \"Test size:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49563f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Spark_LogisticRegression\"):\n",
    "\n",
    "    # Assemble features\n",
    "    assembler = VectorAssembler(inputCols=[f\"feature_{i}\" for i in range(64)], outputCol=\"features\")\n",
    "\n",
    "    lr = LogisticRegression(maxIter=50, regParam=0.01, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "    start = time.time()\n",
    "    model = pipeline.fit(train_df)\n",
    "    end = time.time()\n",
    "\n",
    "    preds = model.transform(test_df)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    acc = evaluator.evaluate(preds)\n",
    "    f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"maxIter\", 50)\n",
    "    mlflow.log_param(\"regParam\", 0.01)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"train_time_sec\", end - start)\n",
    "    \n",
    "     # Log model\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    log_confusion_matrix(preds, \"Spark_LogReg\")\n",
    "\n",
    "print(\"Spark_LogisticRegression -> acc:\", acc, \"f1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Spark_RandomForest\"):\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[f\"feature_{i}\" for i in range(64)], outputCol=\"features\")\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100, maxDepth=10)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    start = time.time()\n",
    "    model = pipeline.fit(train_df)\n",
    "    end = time.time()\n",
    "    preds = model.transform(test_df)\n",
    "\n",
    "    acc = evaluator.evaluate(preds)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "\n",
    "    mlflow.log_param(\"model\", \"RandomForest\")\n",
    "    mlflow.log_param(\"numTrees\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 10)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"train_time_sec\", end - start)\n",
    "\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "    \n",
    "    log_confusion_matrix(preds, \"Spark_RandomForest\")\n",
    "\n",
    "print(\"Spark_RandomForest -> acc:\", acc, \"f1:\", f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Spark_GradientBoosting\"):\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[f\"feature_{i}\" for i in range(64)], outputCol=\"features\")\n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=100, maxDepth=5)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "    start = time.time()\n",
    "    model = pipeline.fit(train_df)\n",
    "    end = time.time()\n",
    "\n",
    "    preds = model.transform(test_df)\n",
    "\n",
    "    acc = evaluator.evaluate(preds)\n",
    "    f1 = f1_eval.evaluate(preds)\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"GradientBoosting\")\n",
    "    mlflow.log_param(\"maxIter\", 100)\n",
    "    mlflow.log_param(\"maxDepth\", 5)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    mlflow.log_metric(\"train_time_sec\", end - start)\n",
    "\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    log_confusion_matrix(preds, \"Spark_GradientBoosting\")\n",
    "\n",
    "print(\"Spark_GradientBoosting -> acc:\", acc, \"f1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (mlflow-venv)",
   "language": "python",
   "name": "mlflow-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
